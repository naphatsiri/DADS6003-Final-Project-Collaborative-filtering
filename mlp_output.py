# -*- coding: utf-8 -*-
"""MLP_Output.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1oG4qDtOmg416MXGcDe1a1lFButRZNR9a
"""

#MLP Model_uptodate_22.00
import numpy as np
import tensorflow as tf
from tensorflow.keras import backend as K
from tensorflow.keras.models import Model
from tensorflow.keras.layers import Embedding, Input, Flatten, Dense, Multiply, Concatenate
from tensorflow.keras.optimizers import Adagrad, Adam, SGD, RMSprop
from tensorflow.keras.regularizers import l2
from Dataset import Dataset  # Ensure this script exists and is uploaded
from evaluate import evaluate_model  # Ensure this script exists and is uploaded
from time import time
import argparse
import os

#################### Arguments ####################
def parse_args():
    class Args:
        path = '/content/Data/'  # Adjust this path as needed for your dataset location
        dataset = 'ml-1m'
        epochs = 20
        batch_size = 256
        num_factors = 8
        regs = '[0,0]'
        layers = '[64,32,16,8]'  # Only used for MLP
        reg_layers = '[0,0,0,0]'  # Only used for MLP
        num_neg = 4
        lr = 0.001
        learner = 'adam'
        verbose = 1
        out = 1
        model_type = 'MLP'  # Choose between 'GMF' and 'MLP'

    return Args()

#################### GMF Model ####################
def get_gmf_model(num_users, num_items, latent_dim, regs=[0, 0]):
    user_input = Input(shape=(1,), dtype='int32', name='user_input')
    item_input = Input(shape=(1,), dtype='int32', name='item_input')

    # Embedding layers
    MF_Embedding_User = Embedding(input_dim=num_users, output_dim=latent_dim, name='user_embedding',
                                  embeddings_initializer='normal', embeddings_regularizer=l2(regs[0]), input_length=1)
    MF_Embedding_Item = Embedding(input_dim=num_items, output_dim=latent_dim, name='item_embedding',
                                  embeddings_initializer='normal', embeddings_regularizer=l2(regs[1]), input_length=1)

    user_latent = Flatten()(MF_Embedding_User(user_input))
    item_latent = Flatten()(MF_Embedding_Item(item_input))

    # Element-wise product
    predict_vector = Multiply()([user_latent, item_latent])

    prediction = Dense(1, activation='sigmoid', kernel_initializer='lecun_uniform', name='prediction')(predict_vector)

    model = Model(inputs=[user_input, item_input], outputs=prediction)
    return model

#################### MLP Model ####################
def get_mlp_model(num_users, num_items, layers, reg_layers):
    user_input = Input(shape=(1,), dtype='int32', name='user_input')
    item_input = Input(shape=(1,), dtype='int32', name='item_input')

    # Embedding layers
    MLP_Embedding_User = Embedding(input_dim=num_users, output_dim=layers[0] // 2, name='user_embedding',
                                   embeddings_initializer='normal', embeddings_regularizer=l2(reg_layers[0]), input_length=1)
    MLP_Embedding_Item = Embedding(input_dim=num_items, output_dim=layers[0] // 2, name='item_embedding',
                                   embeddings_initializer='normal', embeddings_regularizer=l2(reg_layers[0]), input_length=1)

    user_latent = Flatten()(MLP_Embedding_User(user_input))
    item_latent = Flatten()(MLP_Embedding_Item(item_input))

    # Concatenate user and item embeddings
    vector = Concatenate()([user_latent, item_latent])

    # Fully connected layers
    for idx in range(1, len(layers)):
        vector = Dense(layers[idx], activation='relu', kernel_regularizer=l2(reg_layers[idx]), name=f"layer{idx}")(vector)

    prediction = Dense(1, activation='sigmoid', kernel_initializer='lecun_uniform', name='prediction')(vector)

    model = Model(inputs=[user_input, item_input], outputs=prediction)
    return model

#################### Main ####################
# def get_train_instances function
def get_train_instances(train, num_negatives):
    """
    Generate training instances from the interaction matrix.

    Args:
    - train: Sparse matrix containing user-item interactions.
    - num_negatives: Number of negative samples per positive instance.

    Returns:
    - user_input: List of user IDs for training.
    - item_input: List of item IDs for training.
    - labels: List of labels (1 for positive, 0 for negative).
    """
    user_input, item_input, labels = [], [], []
    num_users, num_items = train.shape
    for (u, i) in zip(*train.nonzero()):
        # Positive instance
        user_input.append(u)
        item_input.append(i)
        labels.append(1)
        # Negative instances
        for _ in range(num_negatives):
            j = np.random.randint(num_items)
            while train[u, j] != 0:  # Ensure it's a truly negative sample
                j = np.random.randint(num_items)
            user_input.append(u)
            item_input.append(j)
            labels.append(0)
    return user_input, item_input, labels


if __name__ == '__main__':
    args = parse_args()
    num_factors = args.num_factors
    regs = eval(args.regs)
    layers = eval(args.layers)
    reg_layers = eval(args.reg_layers)
    num_negatives = args.num_neg
    learner = args.learner
    learning_rate = args.lr
    epochs = args.epochs
    batch_size = args.batch_size
    verbose = args.verbose
    model_type = args.model_type

    topK = 10
    evaluation_threads = 1  # Set to available CPUs
    print(f"Arguments: {args}")

    # Ensure data directory exists
    dataset_path = os.path.join(args.path, args.dataset)
    if not os.path.exists(dataset_path):
        print(f"Dataset path {dataset_path} does not exist. Upload your dataset to Colab.")
        exit()

    # Load data
    t1 = time()
    dataset = Dataset(dataset_path)
    train, testRatings, testNegatives = dataset.trainMatrix, dataset.testRatings, dataset.testNegatives
    num_users, num_items = train.shape
    print("Data loaded. #user=%d, #item=%d, #train=%d, #test=%d [%.1f s]" %
          (num_users, num_items, train.nnz, len(testRatings), time() - t1))

    # Build model
    if model_type == 'GMF':
        model = get_gmf_model(num_users, num_items, num_factors, regs)
    elif model_type == 'MLP':
        model = get_mlp_model(num_users, num_items, layers, reg_layers)
    else:
        raise ValueError(f"Unknown model type: {model_type}")

    # Compile model
    if learner.lower() == "adagrad":
        model.compile(optimizer=Adagrad(learning_rate=learning_rate), loss='binary_crossentropy')
    elif learner.lower() == "rmsprop":
        model.compile(optimizer=RMSprop(learning_rate=learning_rate), loss='binary_crossentropy')
    elif learner.lower() == "adam":
        model.compile(optimizer=Adam(learning_rate=learning_rate), loss='binary_crossentropy')
    else:
        model.compile(optimizer=SGD(learning_rate=learning_rate), loss='binary_crossentropy')

    # Evaluate initial performance
    t1 = time()
    (hits, ndcgs) = evaluate_model(model, testRatings, testNegatives, topK, evaluation_threads)
    hr, ndcg = np.array(hits).mean(), np.array(ndcgs).mean()
    print('Init: HR = %.4f, NDCG = %.4f [%.1f s]' % (hr, ndcg, time() - t1))

    # Train model
    best_hr, best_ndcg, best_iter = hr, ndcg, -1
    for epoch in range(epochs):
        t1 = time()
        # Generate training instances
        user_input, item_input, labels = get_train_instances(train, num_negatives)

        # Train
        hist = model.fit([np.array(user_input), np.array(item_input)],  # Input
                         np.array(labels),  # Labels
                         batch_size=batch_size, epochs=1, verbose=0, shuffle=True)
        t2 = time()

        # Evaluate
        if epoch % verbose == 0:
            (hits, ndcgs) = evaluate_model(model, testRatings, testNegatives, topK, evaluation_threads)
            hr, ndcg, loss = np.array(hits).mean(), np.array(ndcgs).mean(), hist.history['loss'][0]
            print('Epoch %d [%.1f s]: HR = %.4f, NDCG = %.4f, loss = %.4f [%.1f s]' %
                  (epoch, t2 - t1, hr, ndcg, loss, time() - t2))
            if hr > best_hr:
                best_hr, best_ndcg, best_iter = hr, ndcg, epoch
                if args.out > 0:
                    model.save(f'{args.dataset}_{model_type}_model.h5')

    print("End. Best Iteration %d:  HR = %.4f, NDCG = %.4f." % (best_iter, best_hr, best_ndcg))